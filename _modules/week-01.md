---
title: Week 1
---

Jan 9
: **Lecture**{: .label .label-blue }[Introduction](#)
  : Percy Liang

Jan 11
: **Speaker**{: .label .label-green }[FlashAttention](#)   
  : [Tri Dao](https://tridao.me) (Stanford)
: ***Reading materials***
<!-- ***Reading materials***:   -->
<!-- 2 short blogposts on Transformer and GPUs (and optionally a paper on attention and a performance guide from Nvidia). -->
<!-- - **Transformers** -->
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Attention is All You Need](https://arxiv.org/abs/1706.03762) (Optional, but recommended!)  
<!-- - **Deep learning on GPUs**   -->
- [Making Deep Learning Go Brrrr From First Principles.](https://horace.io/brrr_intro.html)
- [NVIDIA deep learning performance guide.](https://docs.nvidia.com/deeplearning/performance/index.html) (Optional)

  

 
  
  
